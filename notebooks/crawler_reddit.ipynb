{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80810ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def from_reddit(self, url: str, qtd_chamadas: int=5) -> list[dict]:\n",
    "        json_result_list = []\n",
    "        tentativa = qtd_chamadas\n",
    "        aux = 0\n",
    "\n",
    "        # Primeira requisição\n",
    "        response = requests.get(url)\n",
    "        json_object = response.json()\n",
    "\n",
    "        next_page = json_object['data']['after']\n",
    "        data_exists = json_object['data']\n",
    "\n",
    "        for item in json_object['data']['children']:\n",
    "            aux = aux+1\n",
    "        for page in range(aux):\n",
    "            json_result_list.append(json_object['data']['children'][page]['data']['selftext'])\n",
    "\n",
    "        # Loop para paginação\n",
    "        while data_exists:\n",
    "            if tentativa > 0:\n",
    "                time.sleep(5)\n",
    "                print(\"Next page:\", next_page)\n",
    "                response = requests.get(url, params={'after': next_page})\n",
    "                json_object = response.json()\n",
    "\n",
    "                # Pega a chave selftext de cada post, por página\n",
    "                aux = 0\n",
    "                for item in json_object['data']['children']:\n",
    "                    aux = aux+1\n",
    "                for page in range(aux):\n",
    "                    json_result_list.append(json_object['data']['children'][page]['data']['selftext'])\n",
    "\n",
    "                next_page = json_object['data'].get('after')\n",
    "                data_exists = json_object['data']\n",
    "                tentativa = tentativa - 1\n",
    "            else: break\n",
    "        return json_result_list\n",
    "          \n",
    "    def from_website(self, references_links: pd.DataFrame) -> pd.DataFrame:\n",
    "        options = Options()\n",
    "        options.add_argument('--headless=new')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--log-level=3')\n",
    "        extracted_text = []\n",
    "\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "\n",
    "        for item in references_links:\n",
    "            if references_links['journal'] == 'einvestidor':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '/html/body/div[2]/article/div[3]')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "            elif references_links['journal'] == 'cnnbrasil':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '/html/body/div[2]/article/div[3]')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "            elif references_links['journal'] == 'valorinveste':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/main/div[2]/div/article/div[1]/div[3]')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "            elif references_links['journal'] == 'agenciabrasil':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/main/div[2]/div/div/div[1]/div/div[5]')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            elif references_links['journal'] == 'estadao':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/main/article')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            elif references_links['journal'] == 'f5folha':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[9]/div/div[1]/article/div[3]')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            elif references_links['journal'] == 'poder360':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/main/div[2]/div/div[1]/article/div[1]/div[3]')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            elif references_links['journal'] == 'g1':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[2]/main/div[4]/article')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            elif references_links['journal'] == 'serasa':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/div/div/main/div/div/div/div/div/div/div[2]/div[1]/div/div[2]/div/div[1]/div[5]')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            elif references_links['journal'] == 'noticiasuol':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/main/article/div[1]/div[2]/div/div/div')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            elif references_links['journal'] == 'em':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[2]/div[5]/div[1]/div[1]/div/div/div/div[4]/div[1]/div/div[4]/div[2]')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            elif references_links['journal'] == 'unicesumar':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/div/div[1]')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            elif references_links['journal'] == 'contraponto':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[1]/div/div/div[2]/div/div[1]/article/div[2]')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            elif references_links['journal'] == 'cm7brasil':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/main/div/div[2]/div/article')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            elif references_links['journal'] == 'folhafinanceira':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[2]/div/main/div/div/div[1]/div[1]/article/div[4]')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "            elif references_links['journal'] == 'jornalcontabil':\n",
    "                driver.get(references_links['link'])\n",
    "                time.sleep(5)\n",
    "                try:\n",
    "                    text_element = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, '/html/body/div[4]/div[2]/div/div/article/div/div/div[1]/div/div[1]/div/div[2]/div/div[3]/div/div[1]/div[1]')))\n",
    "                    extracted_text.append(text_element.text)\n",
    "                except Exception as e:\n",
    "                    print(\"Erro ao carregar comentários:\", e)\n",
    "                finally:\n",
    "                    driver.quit()\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
